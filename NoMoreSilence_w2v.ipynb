{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No More Silence -- Word2Vec of Documents by Year\n",
    "\n",
    "The following code uses Spark's implementation of Word2Vec on the No More Silence documents by year on Information Commons.\n",
    "\n",
    "By convention, during preprocessing, we filtered out all documents belonging to more than 3 years. All documents spanning 3 or less years have all of their sentences mapped to each year the document spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1564501043604_0006</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-12-65.us-west-2.compute.internal:20888/proxy/application_1564501043604_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-12-59.us-west-2.compute.internal:8042/node/containerlogs/container_1564501043604_0006_01_000001/millsh\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# Load modules\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "from json import dumps\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Loads data from my AWS S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from S3\n",
    "raw = sc.textFile(\"s3://bchsi-spark02/home/millsh/sents.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Raw Data\n",
    "\n",
    "Input file is formatted as follows: Each line contains a sentence and the year(s) a sentence belongs.\n",
    "Year(s) and sentence are delimited by tab (\"\\t\"), words of a sentence are delimited by space (\" \"), and years are delimited by dash (\"-\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_split = raw.map(\n",
    "    lambda line: (\n",
    "        line.split(\"\\t\")[0].split(\"-\"), \n",
    "        line.split(\"\\t\")[1].split(\" \")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Word2Vec Function\n",
    "\n",
    "Function to run Spark's Word2vec for a given year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v(year):\n",
    "    # Filter by year, and select words\n",
    "    sents_filtered = raw_split.filter(lambda row: str(year) in row[0]) \\\n",
    "                     .map(lambda row: Row(row[1]))\n",
    "    \n",
    "    # Create Spark DF of words\n",
    "    df = spark.createDataFrame(sents_filtered, [\"text\"])\n",
    "    \n",
    "    # Run Word2Vec\n",
    "    word2Vec = Word2Vec(vectorSize=128, minCount=3, maxIter = 50, \n",
    "                        inputCol=\"text\", outputCol=\"result\")\n",
    "    model = word2Vec.fit(df)\n",
    "    \n",
    "    # Return dictionaty of embeddings (keys are words, and values are word vecs)\n",
    "    return { text : [ e for e in vector ] for text, vector in model.getVectors().collect() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Word2Vec by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1982 59.433645248413086\n",
      "1983 14.445246458053589\n",
      "1984 140.23983120918274\n",
      "1985 208.82717609405518\n",
      "1986 176.01878094673157\n",
      "1987 204.8292965888977\n",
      "1988 257.15314650535583\n",
      "1989 184.97558736801147\n",
      "1990 432.4536256790161\n",
      "1991 255.13430619239807\n",
      "1992 289.4339325428009\n",
      "1993 324.9008867740631\n",
      "1994 218.08603239059448\n",
      "1995 83.61162328720093"
     ]
    }
   ],
   "source": [
    "# Relavent years (bulk of our data)\n",
    "years = range(1982,1996)\n",
    "results = {}\n",
    "\n",
    "for year in years:\n",
    "    t0 = time()\n",
    "    \n",
    "    # Run Word2vec, and store embeddings in dictionary indexed by year\n",
    "    results[year] = w2v(year)\n",
    "    \n",
    "    # Print processintg time in sec\n",
    "    print(year, time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272200019"
     ]
    }
   ],
   "source": [
    "# Save Results in JSON format\n",
    "with open(\"/tmp/w2vRes128-100.json\", \"w+\") as ofile:\n",
    "    ofile.write(dumps(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
